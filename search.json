[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joshua Sweren",
    "section": "",
    "text": "Joshua Sweren is a graduate student in the Data Science and Analytics program at Georgetown University, anticipated to receive a Master of Science degree in December 2025. He previously received his B.S. in Statistics with minors in Computer Science and Mathematics from the George Washington University in 2020. Mr. Sweren has remained in the Washington, D.C. area and previously worked for over five years in federal IT consulting, working as a Consultant and Business Analyst for multiple firms. Through work and education, he has developed proficiencies in scripting and programming languages such as Python, R, and SQL, as well as business intelligence tools such as Tableau, Power BI, and Sharepoint. He is adept at solving complex problems to support strategic decision-making in dynamic environments and looks forward to applying his skills as a Data Scientist in the future."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "5000-website/dimensionality_reduction/dimensionality_reduction.html",
    "href": "5000-website/dimensionality_reduction/dimensionality_reduction.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "In this section, we will be using dimensionality reduction techniques to gain insights on public transit data for major US cities. The dataset for this comes from the American Public Transit Association Ridership Report, which contains details about public transit ridership from 2022. 1\nWhile this dataset does contain some information on the volume of ridership, unsupervised learning is generally done without a target variable or known relationships within the data. Thus, the features of this dataset are city population, city area (square miles), average cost per trip (dollars), average fare per trip (dollars), and average miles per trip, where the observation unit for each record is an individual city. In practice, all of these features could be factors in understanding the health of a public transit system, as they all either provide information on the city itself, the conditions for the riders, or the cost for the city. Thus, the objective of dimensionality reduction is to discover things about these features and how they interact with one another.\nTo do dimensionality reduction, there are two common methods: Principal Component Analysis (PCA) and T-distributed Stochastic Neighbor Embedding (TSNE). Both will be applied to this dataset. We will be using the following Python libraries to accomplish these:\n\nnumpy for obtaining eigenvalues and eigenvectors\nsklearn for implementing PCA and TSNE\nmatplotlib and seaborn for visualizations"
  },
  {
    "objectID": "5000-website/dimensionality_reduction/dimensionality_reduction.html#objectives",
    "href": "5000-website/dimensionality_reduction/dimensionality_reduction.html#objectives",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "In this section, we will be using dimensionality reduction techniques to gain insights on public transit data for major US cities. The dataset for this comes from the American Public Transit Association Ridership Report, which contains details about public transit ridership from 2022. 1\nWhile this dataset does contain some information on the volume of ridership, unsupervised learning is generally done without a target variable or known relationships within the data. Thus, the features of this dataset are city population, city area (square miles), average cost per trip (dollars), average fare per trip (dollars), and average miles per trip, where the observation unit for each record is an individual city. In practice, all of these features could be factors in understanding the health of a public transit system, as they all either provide information on the city itself, the conditions for the riders, or the cost for the city. Thus, the objective of dimensionality reduction is to discover things about these features and how they interact with one another.\nTo do dimensionality reduction, there are two common methods: Principal Component Analysis (PCA) and T-distributed Stochastic Neighbor Embedding (TSNE). Both will be applied to this dataset. We will be using the following Python libraries to accomplish these:\n\nnumpy for obtaining eigenvalues and eigenvectors\nsklearn for implementing PCA and TSNE\nmatplotlib and seaborn for visualizations"
  },
  {
    "objectID": "5000-website/dimensionality_reduction/dimensionality_reduction.html#implementation",
    "href": "5000-website/dimensionality_reduction/dimensionality_reduction.html#implementation",
    "title": "Dimensionality Reduction",
    "section": "Implementation",
    "text": "Implementation\n\nDimensionality Reduction with PCA\nNote: Some of this code is repurposed from DSAN-5000 Week 10 slides.\n\n\nCode\nimport pandas as pd\ncities = pd.read_csv('../data/cleaned_data/apta_cities_cleaned.csv')\ncities = cities.drop(columns=['Unnamed: 0'])\ncities.head()\n\n\n\n\n\n\n\n\n\nCity\nPopulation\nArea\nCost_per_trip\nFare_per_trip\nMiles_per_trip\nTotal_trips\nTrips_per_capita\n\n\n\n\n0\nSeattle--Tacoma, WA\n3544011\n982.52\n13.906032\n1.570667\n5.786344\n130093841\n36.708080\n\n\n1\nSpokane, WA\n447279\n171.67\n13.433827\n0.988308\n4.772569\n6995911\n15.641045\n\n\n2\nYakima, WA\n133145\n55.77\n19.720093\n1.112531\n5.179168\n513484\n3.856577\n\n\n3\nEugene, OR\n270179\n73.49\n10.851494\n2.753356\n3.684118\n5296214\n19.602612\n\n\n4\nPortland, OR--WA\n2104238\n519.30\n10.804361\n1.025659\n4.011388\n56312874\n26.761647\n\n\n\n\n\n\n\nA crucial step for PCA is first obtaining eigenvalues and eigenvectors to figure out the properties of the feature matrix. This process is printed below.\n\n\nCode\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nX = cities.drop(columns=['City']).to_numpy()\n\nprint('NUMERIC MEAN:\\n',np.mean(X,axis=0))\nprint(\"X SHAPE\",X.shape)\nprint(\"NUMERIC COV:\")\nprint(np.cov(X.T))\n\nfrom numpy import linalg as LA\nw, v1 = LA.eig(np.cov(X.T))\nprint(\"\\nCOV EIGENVALUES:\",w)\nprint(\"COV EIGENVECTORS (across rows):\")\nprint(v1.T)\n\n\nNUMERIC MEAN:\n [7.63817374e+05 2.54954371e+02 1.62164796e+01 1.69764181e+00\n 6.02033451e+00 2.03019959e+07 9.62142228e+00]\nX SHAPE (286, 7)\nNUMERIC COV:\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n[[ 3.00348049e+12  6.21976504e+08 -2.68509287e+06 -2.80714130e+05\n  -3.74964286e+05  2.24741175e+14  1.29984354e+07]\n [ 6.21976504e+08  1.60662563e+05 -7.09356971e+02 -8.34065149e+01\n  -1.18548286e+02  3.87177727e+10  2.29845351e+03]\n [-2.68509287e+06 -7.09356971e+02  1.12521769e+02  1.14344019e+01\n   1.54309551e+01 -1.51824020e+08 -4.69434589e+01]\n [-2.80714130e+05 -8.34065149e+01  1.14344019e+01  1.07182685e+01\n   6.17463373e+00 -4.36561531e+06 -2.83807355e+00]\n [-3.74964286e+05 -1.18548286e+02  1.54309551e+01  6.17463373e+00\n   2.61947556e+01 -1.78488703e+07 -5.91059175e+00]\n [ 2.24741175e+14  3.87177727e+10 -1.51824020e+08 -4.36561531e+06\n  -1.78488703e+07  2.85052428e+16  1.49479496e+09]\n [ 1.29984354e+07  2.29845351e+03 -4.69434589e+01 -2.83807355e+00\n  -5.91059175e+00  1.49479496e+09  1.65428342e+02]]\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n\nCOV EIGENVALUES: [2.85070148e+16 1.23149854e+12 2.66254943e+04 1.39995139e+02\n 5.92802397e+01 8.14460157e+00 2.44793107e+01]\nCOV EIGENVECTORS (across rows):\n[[-7.88430047e-03 -1.35831385e-06  5.32642483e-09  1.53214669e-10\n   6.26206285e-10 -9.99968918e-01 -5.24380050e-08]\n [ 9.99968885e-01  2.57162006e-04 -1.20827045e-06 -1.99988539e-07\n  -1.90196564e-07 -7.88430056e-03  9.84669318e-07]\n [ 2.57165690e-04 -9.99987334e-01  4.54144922e-03  5.33265824e-04\n   1.28310685e-03 -6.69356967e-07  1.64600913e-03]\n [-8.27843413e-07 -2.91942735e-03 -8.03111034e-01 -8.55421446e-02\n  -1.33834128e-01 -2.39959801e-08  5.74260491e-01]\n [ 1.14210098e-06 -4.07870902e-03 -5.48463614e-01 -1.02974170e-01\n  -1.47054608e-01  3.63317166e-08 -8.16665348e-01]\n [-1.27488408e-07  2.07273385e-04  7.32435013e-02 -9.59322603e-01\n   2.71686187e-01 -6.13010609e-11  2.28496311e-02]\n [-5.09559302e-08  2.50913689e-04 -2.20941387e-01  2.48553256e-01\n   9.41620060e-01  2.26566582e-09 -5.25145905e-02]]\n\n\nUpon obtaining the properties of our dataset, we can now use sklearn to implement PCA. The steps for this are as follows:\n\nNormalize the feature matrix using the StandardScalar() function\nSort the eigenvectors in order to prioritize principal components\nFind the cumulative sum of the principal components to find the proportion of the variance that can be explained by each number of features\nPlot this cumulative distribution\n\nNote: Some of this code is repurposed from https://www.geeksforgeeks.org/reduce-data-dimentionality-using-pca-python/ 2\n\n\nCode\nfrom sklearn.preprocessing import StandardScaler\n\nX = StandardScaler().fit_transform(X)\ncov = (X.T @ X) / (X.shape[0] - 1)\nw, v1 = np.linalg.eig(cov)\n\nidx = np.argsort(w, axis=0)[::-1]\nsorted_eig_vectors = v1[:, idx]\n\ncumsum = np.cumsum(w[idx]) / np.sum(w[idx])\nxint = range(1, len(cumsum) + 1)\nplt.plot(xint, cumsum)\n\nplt.xlabel(\"Number of components\")\nplt.ylabel(\"Cumulative explained variance\")\nplt.xticks(xint)\nplt.xlim(1, 5, 1)\n\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n\n\n/var/folders/z5/l6g0391s0qg3vsbnvl7y81n80000gn/T/ipykernel_81423/3215643524.py:17: MatplotlibDeprecationWarning: Passing the emit parameter of set_xlim() positionally is deprecated since Matplotlib 3.6; the parameter will become keyword-only two minor releases later.\n  plt.xlim(1, 5, 1)\n\n\n(1.0, 5.0)\n\n\n\n\n\nFrom the plot above, we can see that greater than 95% of the cumulative explained variance is covered by 4 components. Therefore, it is reasonable to select that as the number of principal components. A good way to check the efficacy of this is to plot the covariance using seaborn from before and after PCA. Below is the covariance matrix of the feature dataset, which clearly shows a significant amount of covariance between a few of the variables.\n\n\nCode\nimport seaborn as sns\nX_df = pd.DataFrame(X).rename(columns={0: 'Population', 1: 'Area', 2: 'Cost_per_trip', 3: 'Fare_per_trip', 4: 'Miles_per_trip'})\nsns.heatmap(X_df.corr())\n\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n\nCode\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=4)\npca.fit(X)\ndata_pca = pca.transform(X)\ndata_pca = pd.DataFrame(data_pca,columns=['PC1','PC2','PC3','PC4'])\ndata_pca.head()\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\n\n\n\n\n0\n3.033796\n0.350153\n0.122935\n0.019467\n\n\n1\n0.070137\n-0.474524\n0.321328\n-0.191796\n\n\n2\n-0.723213\n-0.152967\n-0.292844\n-0.111154\n\n\n3\n0.051012\n-0.449029\n0.788093\n-0.746024\n\n\n4\n1.584307\n-0.407123\n0.325299\n-0.194906\n\n\n\n\n\n\n\nAfter applying PCA, below is another heatmap showing the covariance between principal components, which greatly highlights the usefulness of this process. There is essentially no covariance between principal components, indicating that the 4 component selection was effective in summarizing data.\n\n\nCode\nsns.heatmap(data_pca.corr())\n\n\n&lt;Axes: &gt;\n\n\n\n\n\nFinally, below is a plot to visualize the data after selecting principal components.\n\n\nCode\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib widget\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(data_pca['PC2'],data_pca['PC3'],data_pca['PC4'], c=data_pca['PC1'])\nax.set_title(\"3D Plot of Principal Components\")\nax.set_xlabel('PC2')\nax.set_ylabel('PC3')\nax.set_zlabel('PC4')\nplt.show()\n\n\n\n\n\n\n\nDimensionality Reduction with TSNE\nFor implementing TSNE, we will once again be using sklearn. The TSNE() function unfortunately limits to three components, so this will mainly be used for parameter tuning to analyze different perplexities and how they affect our visualizations. The results of a couple of these implementations are below:\n\n\nCode\nfrom sklearn.manifold import TSNE\nX_embedded = TSNE(n_components=3, learning_rate='auto',init='random', perplexity=1).fit_transform(X)\n\n# EXPLORE RESULTS\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:4,:])\n\n# PLOT \nplt.scatter(X_embedded[:,0],X_embedded[:,1], alpha=0.5)\nplt.show()\n\n\nRESULTS\nshape :  (286, 3)\nFirst few points : \n [[  1.4826158   8.286032    4.584907 ]\n [ 11.727755   -5.2766175 -22.360485 ]\n [  5.0308733   5.835106  -26.438314 ]\n [  3.2732215  -2.770161    4.2295136]]\n\n\n\n\n\n\n\nCode\nX_embedded = TSNE(n_components=3, learning_rate='auto',init='random', perplexity=10).fit_transform(X)\n\n# EXPLORE RESULTS\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:4,:])\n\n\nRESULTS\nshape :  (286, 3)\nFirst few points : \n [[ 7.439387    7.215633    3.3639798 ]\n [ 0.01349061  5.4039626  -0.5400352 ]\n [-2.5031161  -4.7561264  -3.151865  ]\n [ 0.34158447  7.827049   -3.8418515 ]]\n\n\n\n\nCode\n# PLOT \nplt.scatter(X_embedded[:,0],X_embedded[:,1], alpha=0.5)\nplt.show()"
  },
  {
    "objectID": "5000-website/dimensionality_reduction/dimensionality_reduction.html#evaluation",
    "href": "5000-website/dimensionality_reduction/dimensionality_reduction.html#evaluation",
    "title": "Dimensionality Reduction",
    "section": "Evaluation",
    "text": "Evaluation\nUltimately, for this application, PCA proved to be a more useful process for understanding relationships within the feature matrix of our data. In general, PCA is ideal for preserving variance in the data, while TSNE preserves relationships more effectively. The crucial difference between the two is that PCA is a linear technique while TSNE is non-linear. For a dataset like this one, where ordering of data points is not a factor, the features were separable from one another, and the initial dimensionality is quite low, PCA is likely to be more effective."
  },
  {
    "objectID": "5000-website/dimensionality_reduction/dimensionality_reduction.html#footnotes",
    "href": "5000-website/dimensionality_reduction/dimensionality_reduction.html#footnotes",
    "title": "Dimensionality Reduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Raw monthly ridership (no adjustments or estimates),” Raw Monthly Ridership (No Adjustments or Estimates) | FTA, https://www.transit.dot.gov/ntd/data-product/monthly-module-raw-data-release (accessed Nov. 14, 2023).↩︎\n“Reduce data dimensionality using PCA - Python,” GeeksforGeeks, https://www.geeksforgeeks.org/reduce-data-dimentionality-using-pca-python/ (accessed Nov. 14, 2023).↩︎"
  },
  {
    "objectID": "5000-website/naive_bayes/naive_bayes.html",
    "href": "5000-website/naive_bayes/naive_bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes classification is a group of supervised learning algorithms which use Bayes’ theorem as their foundation. The “naive” aspect of these algorithms is the assumption that every pair of features are conditionally independent. 1 This can be better understood via the Bayes’ Theorem formula and how it applies in Naive Bayes. Given an output variable \\(y\\) and several feature variables \\(x_i\\), Bayes’ Theorem posits the following:\n\\(P\\left(y | x_1, \\ldots, x_n\\right)=\\frac{P(y) P\\left(x_1, \\ldots, x_n | y\\right)}{P\\left(x_1, \\ldots, x_n\\right)}\\)\nThe assumption that all feature variables are conditionally independent can be represented with the formula\n\\(P\\left(x_i \\mid y, x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n\\right)=P\\left(x_i \\mid y\\right)\\)\nwhich essentially states that the values of each \\(x_i\\) depend solely on \\(y\\), not the values from any other feature variable. Thus, we can substitute the products of all conditional probabilities into Bayes’ Theorem to represent the relationship as\n\\(P\\left(y \\mid x_1, \\ldots, x_n\\right)=\\frac{P(y) \\prod_{i=1}^n P\\left(x_i \\mid y\\right)}{P\\left(x_1, \\ldots, x_n\\right)}\\).\nUsing this foundation, Naive Bayes classification algorithms attempt to establish classifiers for a dataset by considering values for each feature and assigning probabilities that it belongs in a certain class of the specified output variable. To do this, the algorithm will first take in data from a test dataset. When executing Naive Bayes classification, it is generally recommended to partition cleaned data such that 80% of it belongs in the test dataset. The algorithm will iterate over the feature variables and observe the output, ultimately creating a model for the extents to which the feature variables predict the output classifications.\nDue to the foundation being on Bayes’ Theorem, these methods are probabilistic in nature. That is, rather than being able to decisively classify new data, the algorithms work based on conditional probabilities that records belong to a particular class.\nDiscerning when to use the different variants of Naive Bayes classification is crucial for obtaining meaningful results. Firstly, Gaussian Naive Bayes (GNB) assumes that the feature variables follow a normal distribution. Thus, if we have data that we know comes from a normally distributed population, we can use the appropriate likelihood function for parameter estimation. The same applies for other methods of Naive Bayes classification. Other popular algorithms include Multinomial Naive Bayes, Bernoulli Naive Bayes, Categorical Naive Bayes, and others. Essentially, the presumed distribution of the feature variables can be leveraged to give an appropriate likelihood function, which aids in estimating parameters.\n\n\nFor the purposes of our analysis, Naive Bayes can be used to analyze both record and text data. Firstly, the record data we will be using is the Census data which contains survey responses on various demographic features. The purpose of this will be to apply the Categorical Naive Bayes algorithm to features such as sex, race, and marital status to fit a model with a respondant’s method of transportation to commute to work as the output variable. This will allow us to gain insights on which types of people are most affected by changes in public transit policy and service, providing valuable context for how a city’s conditions can impact its residents.\nSecondly, the labeled text data that we will use is the combined dataset of Yelp reviews for WMATA and BART. These revies contain textual responses with accompanying numerical ratings (1-5 stars). Thus, the text will be considered the feature set for Multinomial Naive Bayes, with the ratings deemed the output, as it labels the review with a scale of satisfaction or dissatisfaction with a service. The purpose of this is to analyze public sentiment regarding public transit systems and how that may affect usage or responses to potential policy changes."
  },
  {
    "objectID": "5000-website/naive_bayes/naive_bayes.html#introduction-to-naive-bayes",
    "href": "5000-website/naive_bayes/naive_bayes.html#introduction-to-naive-bayes",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes classification is a group of supervised learning algorithms which use Bayes’ theorem as their foundation. The “naive” aspect of these algorithms is the assumption that every pair of features are conditionally independent. 1 This can be better understood via the Bayes’ Theorem formula and how it applies in Naive Bayes. Given an output variable \\(y\\) and several feature variables \\(x_i\\), Bayes’ Theorem posits the following:\n\\(P\\left(y | x_1, \\ldots, x_n\\right)=\\frac{P(y) P\\left(x_1, \\ldots, x_n | y\\right)}{P\\left(x_1, \\ldots, x_n\\right)}\\)\nThe assumption that all feature variables are conditionally independent can be represented with the formula\n\\(P\\left(x_i \\mid y, x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n\\right)=P\\left(x_i \\mid y\\right)\\)\nwhich essentially states that the values of each \\(x_i\\) depend solely on \\(y\\), not the values from any other feature variable. Thus, we can substitute the products of all conditional probabilities into Bayes’ Theorem to represent the relationship as\n\\(P\\left(y \\mid x_1, \\ldots, x_n\\right)=\\frac{P(y) \\prod_{i=1}^n P\\left(x_i \\mid y\\right)}{P\\left(x_1, \\ldots, x_n\\right)}\\).\nUsing this foundation, Naive Bayes classification algorithms attempt to establish classifiers for a dataset by considering values for each feature and assigning probabilities that it belongs in a certain class of the specified output variable. To do this, the algorithm will first take in data from a test dataset. When executing Naive Bayes classification, it is generally recommended to partition cleaned data such that 80% of it belongs in the test dataset. The algorithm will iterate over the feature variables and observe the output, ultimately creating a model for the extents to which the feature variables predict the output classifications.\nDue to the foundation being on Bayes’ Theorem, these methods are probabilistic in nature. That is, rather than being able to decisively classify new data, the algorithms work based on conditional probabilities that records belong to a particular class.\nDiscerning when to use the different variants of Naive Bayes classification is crucial for obtaining meaningful results. Firstly, Gaussian Naive Bayes (GNB) assumes that the feature variables follow a normal distribution. Thus, if we have data that we know comes from a normally distributed population, we can use the appropriate likelihood function for parameter estimation. The same applies for other methods of Naive Bayes classification. Other popular algorithms include Multinomial Naive Bayes, Bernoulli Naive Bayes, Categorical Naive Bayes, and others. Essentially, the presumed distribution of the feature variables can be leveraged to give an appropriate likelihood function, which aids in estimating parameters.\n\n\nFor the purposes of our analysis, Naive Bayes can be used to analyze both record and text data. Firstly, the record data we will be using is the Census data which contains survey responses on various demographic features. The purpose of this will be to apply the Categorical Naive Bayes algorithm to features such as sex, race, and marital status to fit a model with a respondant’s method of transportation to commute to work as the output variable. This will allow us to gain insights on which types of people are most affected by changes in public transit policy and service, providing valuable context for how a city’s conditions can impact its residents.\nSecondly, the labeled text data that we will use is the combined dataset of Yelp reviews for WMATA and BART. These revies contain textual responses with accompanying numerical ratings (1-5 stars). Thus, the text will be considered the feature set for Multinomial Naive Bayes, with the ratings deemed the output, as it labels the review with a scale of satisfaction or dissatisfaction with a service. The purpose of this is to analyze public sentiment regarding public transit systems and how that may affect usage or responses to potential policy changes."
  },
  {
    "objectID": "5000-website/naive_bayes/naive_bayes.html#preparing-data-for-naive-bayes",
    "href": "5000-website/naive_bayes/naive_bayes.html#preparing-data-for-naive-bayes",
    "title": "Naive Bayes",
    "section": "Preparing Data for Naive Bayes",
    "text": "Preparing Data for Naive Bayes\n\nPreparing Record Data\nTo prepare this data, the first step is to filter out columns that will not be fit in the Categorical Naive Bayes algorithm.\n\n\nCode\nfrom codecs import ignore_errors\nimport comm\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport shutil\n\ncommute=pd.read_csv(\"../data/cleaned_data/commute_cleaned.csv\")\ncommute = commute.drop(columns=['Unnamed: 0','age','income','city_population_00s','transportation_time'])\ncommute['transportation_type'].value_counts()\n\n\ntransportation_type\nPrivate Vehicle    1088363\nWork From Home      269004\nWalk                 36364\nPublic Transit       33144\nOther                16077\nBicycle               5869\nName: count, dtype: int64\n\n\nAs we can see from the distribution of our target variable, transportation_type, this data is very unbalanced. There are significantly more respondents who reported that their primary method of transportation for commuting to work is via Private Vehicle than any other, and the Work From Home group is also over-represented. In cases like this, it is common practice to over- or under-sample from certain groups to balance out the dataset. Without this practice, any supervised learning algorithm will generally predict the over-represented group to reduce the probability of inaccuracies. Therefore, we will under-sample from the two largest groups to bring them within 1,000 responses of the next-most represented group. This can be done via random sample to ensure the data isn’t tampered with too much.\n\n\nCode\ncommute = commute.drop(commute[commute['transportation_type']==\"Private Vehicle\"].sample(n=1052000).index)\ncommute = commute.drop(commute[commute['transportation_type']==\"Work From Home\"].sample(n=233000).index)\ncommute['transportation_type'].value_counts()\n\n\ntransportation_type\nWalk               36364\nPrivate Vehicle    36363\nWork From Home     36004\nPublic Transit     33144\nOther              16077\nBicycle             5869\nName: count, dtype: int64\n\n\nIt is still important to note that this data is no longer representative of the total population, as the groups have been artificially imbalanced. For supervised learning, this is a better balance of the transportation_type variable, but we should keep in mind the altered distribution when making any conclusions.\nThe goal of Naive Bayes is to predictions on your test data using a model that was built using test data. Thus, separating our present data into training and testing datasets is crucial for proper execution. It is generally best practice to partition 80% of this into the test dataset, which is done by specifying test_size in the train_test_split function of sklearn.\nGiven our present data, in order to execute Categorical Naive Bayes, we must encode the categorical data with associated numbers. sklearn, a machine learning software tool in Python which will be used throughout this process, has functions OrdinalEncoder and LabelEncoder to accomplish this. Thus, that will be applied to both train and test datasets once partitioned.\nNote: Code for feature selection with categorical variables partially repurposed from: https://machinelearningmastery.com/feature-selection-with-categorical-data/\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom matplotlib import pyplot\n\ndataset = commute.values\nX = dataset[:, :-1]\ny = dataset[:,-1]\nX = X.astype(str)\n\ndef prepare_inputs(X_train, X_test):\n     oe = OrdinalEncoder()\n     oe.fit(X_train)\n     X_train_enc = oe.transform(X_train)\n     X_test_enc = oe.transform(X_test)\n     return X_train_enc, X_test_enc\n\ndef prepare_targets(y_train, y_test):\n     le = LabelEncoder()\n     le.fit(y_train)\n     y_train_enc = le.transform(y_train)\n     y_test_enc = le.transform(y_test)\n     return y_train_enc, y_test_enc\n \ndef select_features(X_train, y_train, X_test):\n     fs = SelectKBest(score_func=chi2, k='all')\n     fs.fit(X_train, y_train)\n     X_train_fs = fs.transform(X_train)\n     X_test_fs = fs.transform(X_test)\n     return X_train_fs, X_test_fs, fs\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nX_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\ny_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n\n\n\n\nPreparing Text Data\nTo prepare this data, we must first combine WMATA and BART datasets of Yelp reviews. Since this is not an exercise in comparing and contrasting the two, it is necessary to leverage the scalability of Naive Bayes classification by consolidating this data. The next steps are to vectorize the text, randomly shuffle the records, and again split into training and test data sets, where 80% of the data belongs to the training set.\nNote: The code for text data classification is partially repurposed from DSAN-5000 Lab 3.2\n\n\nCode\nyelp=pd.read_csv(\"../data/cleaned_data/yelp_cleaned.csv\")\n\n\n\n\nCode\nreviews=[]\nratings=yelp['Rating']\nfor i in range(0,yelp.shape[0]):\n    keep=\"abcdefghijklmnopqrstuvwxyz \"\n    replace=\".,!;\"\n    tmp=\"\"\n    for char in yelp['Review'][i].replace(\"&lt;br /&gt;\",\"\").lower():\n        if char in replace:\n            tmp+=\" \"\n        if char in keep:\n            tmp+=char\n    tmp=\" \".join(tmp.split())\n    reviews.append(tmp)\n\nratings = np.array(ratings)\n\n\n\n\nCode\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef vectorize(corpus,MAX_FEATURES):\n    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n\n    Xs  =  vectorizer.fit_transform(corpus)   \n    X=np.array(Xs.todense())\n\n    maxs=np.max(X,axis=0)\n    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n\n(x,vocab0)=vectorize(reviews,MAX_FEATURES=10000)\nvocab1 = dict([(value, key) for key, value in vocab0.items()])\ndf2=pd.DataFrame(x)\ns = df2.sum(axis=0)\ndf2=df2[s.sort_values(ascending=False).index[:]]\n#print(df2.head())\ni1=0\nvocab2={}\nfor i2 in list(df2.columns):\n    vocab2[i1]=vocab1[int(i2)]\n    i1+=1\ndf2.columns = range(df2.columns.size)\nx=df2.to_numpy()\n\n\n\n\n\nCode\nimport random\nN=x.shape[0]\nl = [*range(N)]\ncut = int(0.8 * N)\nrandom.shuffle(l)\ntrain_index = l[:cut]\ntest_index = l[cut:]\n\nprint(train_index[0:10])\nprint(test_index[0:10])\n\n\n[427, 660, 1495, 1509, 1076, 1876, 730, 738, 146, 684]\n[393, 525, 2400, 1428, 399, 53, 2188, 1916, 2364, 456]"
  },
  {
    "objectID": "5000-website/naive_bayes/naive_bayes.html#feature-selection",
    "href": "5000-website/naive_bayes/naive_bayes.html#feature-selection",
    "title": "Naive Bayes",
    "section": "Feature Selection",
    "text": "Feature Selection\n\nFeature Selection for Record Data\nThe following code and output provides the feature selection process for our record data.\n\n\nCode\nX_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\nfor i in range(len(fs.scores_)):\n     print(str(commute.columns[i]) + \": \" + str(fs.scores_[i]))\npyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\npyplot.show()\n\n\nsex: 413.7170268487656\nmarital_status: 1073.8327696873725\nrace: 8301.788540304213\nhispanic: 139.35812553260118\nemployment: nan\n\n\n\n\n\n\n\nFeature Selection for Text Data\nThe following code and output provides the feature selection process for our text data.\n\n\nCode\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval)\n\n\n#TEST\nprint(type(x),type(ratings))\nprint(x.shape,ratings.shape)\n(acc_train,acc_test,time_train,time_eval)=train_MNB_model(x,ratings,i_print=True)\n\n\n&lt;class 'numpy.ndarray'&gt; &lt;class 'numpy.ndarray'&gt;\n(2435, 10000) (2435,)\n(2435, 10000) (2435,)\n87.83367556468173 56.05749486652978 0.1518350000000055 0.15289999999998827\n\n\n\n\nCode\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,ratings,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\npartial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\n\n5 50 50 44.866529774127315 43.121149897330596\n10 100 100 51.74537987679672 49.28131416837782\n15 150 150 55.28747433264887 48.870636550308014\n20 200 200 57.13552361396304 49.48665297741273\n25 250 250 58.983572895277206 50.92402464065708\n30 300 300 61.601642710472284 50.308008213552355\n35 350 350 63.29568788501027 50.308008213552355\n40 400 400 63.80903490759754 51.54004106776181\n45 450 450 64.27104722792608 49.89733059548255\n50 500 500 66.4271047227926 51.74537987679672\n55 550 550 67.71047227926078 51.74537987679672\n60 600 600 68.63449691991786 51.129363449691986\n65 650 650 68.99383983572895 50.308008213552355\n70 700 700 69.04517453798768 51.33470225872689\n75 750 750 70.99589322381931 51.74537987679672\n80 800 800 72.07392197125256 50.71868583162218\n85 850 850 73.1006160164271 51.54004106776181\n90 900 900 74.33264887063655 51.95071868583162\n95 950 950 75.05133470225873 51.54004106776181\n100 1000 1000 75.71868583162218 50.92402464065708\n5 3250 3250 88.44969199178645 54.62012320328542\n10 5500 5500 89.5277207392197 55.03080082135524\n15 7750 7750 88.75770020533881 56.67351129363449\n20 10000 10000 87.83367556468173 56.05749486652978\n\n\n\n\nCode\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\ndef plot_results(path_root):\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\nsave_results('../data/naive_bayes'+\"/partial_grid_search\")\nplot_results('../data/naive_bayes'+\"/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\nCode\nx_var=np.var(x,axis=0)\nfrom sklearn.feature_selection import VarianceThreshold\n\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=30\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n#DOESN\"T WORK WELL WITH EDGE VALUES \nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(x)\n    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n\n    (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,ratings,i_print=False)\n             \n    #RECORD \n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\n\nTHRESHOLD = 0.008536336885337658 1167\nTHRESHOLD = 0.016662164808711497 621\nTHRESHOLD = 0.02478799273208534 421\nTHRESHOLD = 0.032913820655459176 301\nTHRESHOLD = 0.041039648578833014 225\nTHRESHOLD = 0.04916547650220686 178\nTHRESHOLD = 0.0572913044255807 143\nTHRESHOLD = 0.06541713234895453 110\nTHRESHOLD = 0.07354296027232837 91\nTHRESHOLD = 0.08166878819570221 76\nTHRESHOLD = 0.08979461611907605 60\nTHRESHOLD = 0.0979204440424499 53\nTHRESHOLD = 0.10604627196582374 41\nTHRESHOLD = 0.11417209988919758 40\nTHRESHOLD = 0.12229792781257141 33\nTHRESHOLD = 0.13042375573594525 29\nTHRESHOLD = 0.1385495836593191 25\nTHRESHOLD = 0.14667541158269293 21\nTHRESHOLD = 0.15480123950606678 16\nTHRESHOLD = 0.1629270674294406 13\nTHRESHOLD = 0.17105289535281445 10\nTHRESHOLD = 0.17917872327618828 8\nTHRESHOLD = 0.18730455119956213 8\nTHRESHOLD = 0.19543037912293598 8\nTHRESHOLD = 0.2035562070463098 8\nTHRESHOLD = 0.21168203496968366 5\nTHRESHOLD = 0.21980786289305748 4\n\n\n\n\nCode\nsave_results(\"../data/naive_bayes\"+\"/variance_threshold\")\nplot_results(\"../data/naive_bayes\"+\"/variance_threshold\")"
  },
  {
    "objectID": "5000-website/naive_bayes/naive_bayes.html#naive-bayes",
    "href": "5000-website/naive_bayes/naive_bayes.html#naive-bayes",
    "title": "Naive Bayes",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nNaive Bayes with Labeled Record Data\nThe following code, output, and comments show the process for Naive Bayes classification on record data.\n\n\nCode\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.naive_bayes import CategoricalNB\nimport scipy\nfrom scipy import stats\nclf = CategoricalNB(force_alpha=True)\nenc = OrdinalEncoder()\nimport warnings\nwarnings.simplefilter('ignore', np.RankWarning)\nwarnings.simplefilter('ignore', np.ComplexWarning)\nwarnings.filterwarnings('ignore', \"Intel MKL ERROR\")\n\nfeature_cols = feature_cols = [c for c in commute.columns if c != 'transportation_type']\nX_train_df = pd.DataFrame(X_train_enc, columns=feature_cols)\nX_test_df = pd.DataFrame(X_test_enc, columns=feature_cols)\nincluded_vars = ['sex','race','marital_status']\nincluded_vars_df = X_train_df[included_vars].copy()\nsex_marital_corr = stats.spearmanr(included_vars_df['sex'], included_vars_df['marital_status']).statistic\nprint(sex_marital_corr)\nsex_race_corr = stats.spearmanr(included_vars_df['sex'], included_vars_df['race']).statistic\nprint(sex_race_corr)\nrace_marital_corr = stats.spearmanr(included_vars_df['race'], included_vars_df['marital_status']).statistic\nprint(race_marital_corr)\nsex_commute_corr = stats.spearmanr(included_vars_df['sex'], y_train).statistic\nprint(sex_commute_corr)\nrace_commute_corr = stats.spearmanr(included_vars_df['race'], y_train).statistic\nprint(race_commute_corr)\nmarital_commute_corr = stats.spearmanr(included_vars_df['marital_status'], y_train).statistic\nprint(marital_commute_corr)\n\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n-0.005800683251658714\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n0.043882164650110994\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n-0.11339339247229406\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n-0.053964880565975454\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.0.05951068089203682\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n-0.017725353197317023\n\n\n\n\nCode\nk = 3\nmean_xx_corr = np.mean([sex_marital_corr,sex_race_corr,race_marital_corr])\nmean_xy_corr = np.mean([sex_commute_corr,race_commute_corr,marital_commute_corr])\nprint(f\"Number of features: {k}\")\nmerit_score_numer = k * np.absolute(mean_xy_corr)\nmerit_score_denom = np.sqrt(k + k * (k + 1) * np.absolute(mean_xx_corr))\nmerit_score_s2 = merit_score_numer / merit_score_denom\nprint(f\"Merit score: {merit_score_s2}\")\n\n\nNumber of features: 3\nMerit score: 0.006703356130666423\n\n\n\n\nCode\nenc.fit(X_train)\nX_train_enc = enc.transform(X_train)\nclf.fit(X_train_enc, y_train)\n\n\nCategoricalNB(force_alpha=True)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.CategoricalNBCategoricalNB(force_alpha=True)\n\n\n\n\nCode\nfrom sklearn.metrics import f1_score\nenc.fit(X_test)\nX_test_enc = enc.transform(X_test)\ntest_predictions = clf.predict(X_test_enc)\nf1_score(y_true = y_test, y_pred = test_predictions, average='weighted')\n\n\n0.2893517844889823\n\n\n\n\nCode\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ncm = confusion_matrix(y_train, clf.predict(X_train_enc), labels=clf.classes_)\ncmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\ncmd.plot()\nplt.show()\n\n\n\n\n\n\n\nCode\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ncm = confusion_matrix(y_test, clf.predict(X_test_enc), labels=clf.classes_)\ncmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\ncmd.plot()\nplt.show()\n\n\n\n\n\nBased on the F-score and these confusion matrices, it appears that this method was only somewhat effective at predicting the labels. An F-score of 0.29 is not very high, but for both the training and testing datasets, the most prevalent value in each column is the one on the diagonal. There were a significant amount of respondents who reported Private Vehicle as their primary method of transportation for which the model predicted that they would work from home. This indicates a potential similarity between these two groups. Overall, this was better than if the predictions were just randomly selected, but it is apparent that this is not an exhaustive predictor of the true label.\n\n\nClass Distribution\nThe record data we will be using comes from the IPUMS dataset containing results from the US Census Bureau. Our target variable will be commute_method, which each respondents answer to what their primary mode of transportation is for work. As this data has been cleaned, this categorical variable is portrayed via text with the following classes, with their respective totals:\n\nPrivate Vehicle - 47254 answers\nPublic Transit - 3365\nBicycle - 593\nWalked - 2322\nWorked from Home - 19013\nOther - 1156\n\nThe following is a visualization of the class distribution:\n\n\nNaive Bayes with Labeled Text Data\nThe following code, output, and comments show the process for Naive Bayes classification on record data.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\n\n# CONVERT DF TO LIST OF STRINGS \ncorpus=yelp[\"Review\"].to_list()\ny2=yelp[\"Rating\"].to_numpy()\n\nprint(\"number of text chunks = \",len(corpus))\nprint(corpus[0:3])\n\n\nnumber of text chunks =  2435\n[\"What do I know, I'm just a stupid tourist who comes into the city once a year and takes 3-4 subway rides each time.A lot cleaner than some of our Philly train stations, that's for sure. Pretty easy to figure out how to get from place to place with Google maps.Love the contactless method - tap and go.\", 'This Company is A Literally A Gangsters Mafia , an Organized Group of Criminals with uniforms !!! and I\\'ll explain exactly why I say that . (1) they are stilling people\\'s money , by damaging their metro cards on purpose !!! In the Busses and subway train station machines !! All that is happening from one reason only that people would buy another card , and another one , and if you send your damaged card to them ? they taking the card and say goodbye to your card and to your money !!! For example in the last six months , I buy in the beginning of the month , a monthly unlimited metro card , pay 127$ !!! And already twelve times I purchased the same card again and again , I mean in a six months !!! why ? Because suddenly and surprisingly the card after two weeks or so is damaging !!! So you guys probably asking how come ? And why every card ?? So I\\'ll say /ask the same question to you !! How come every time a different card in a different trains and buses of MTA damaging after two weeks or sometimes after one week ?? And if it\\'s happening to me , who can tell me how many people in NY , it\\'s happening to them also in the same way ?? And I don\\'t play with the card or use it in not appropriate way , like throw it like a garbage in places that would damage the card !! No No !! it comes out of the wallet in the morning , I use it in the bus and the train and it goes back to the wallet !!! No games with the card !! But any it gets damaged !!! And every month after two or three weeks of use !!! I don\\'t get to use it the whole mont !!! And I use it only two times in a day , on my way to work and when I go back home !!! And I buy Monthly unlimited on purpose , because I live in Long Island , and working in Brooklyn !!! So the cost of day by day to pay 11$ cost to much for me , only for the transportation , plus I need food so I don\\'t want to spend that much money in every month so I use / Buy the Monthly card !!! But this company decided I guess to still the peoples money !!! And again , if it\\'s happening to me ? Who promises that it\\'s not happening to thousands of people in NY every Month ?? And every day ?? One hundred percent for sure that it\\'s happening to a lot more people !!! And by that ? they forcing people to buy their \" new  technology  card \"  and pay more and more money , The OMNI card !!! instead of pay one time monthly purchase metro card and use it for a monthly unlimited uses the people have to pay for each ride separately !!! (2) Look in the Subway tunnels , always dirty like a pothole\\'s !!! Full of Homeless people that bother everyone , including tourists !!! And the company making millions and Billions of money !!! where that money goes ?? Look at their representatives !!! How they treating innocent people that come to ask simple questions about how to use this and how to do that , or for directions to somewhere !!! Go by yourself , Go to places like Flushing NY !!! Or Bronx !! Or Brooklyn NY !!! insulting People , and they get out of it without any punishment !!! why ?? Because they are a Big organized  company!!! And they can bring to the courthouses more lawyers !!  And know exactly that you can not allow to yourself to pay !!! And they pay them with the money they stilling from the people !!! the individual citizens can not do anything against them !!! That\\'s why they are the only one company in NY !!! They are the only Monopoly company in NY , and They Have the union that Belongs to them , and fallow the American History , how the Union started to build up in America !!! And the people in NY city don\\'t have any other choice only to use their services , Because ther is no other companies around in the area to use their services with a normal price to go to places from the five boroughs to Manhattan and back !!! and you can not to talk with them even !!! Like Every other Gangster in the world , they will call the police on you !!! And will put you in jail for at least seven years !!!  But they they can talk to you whatever way they want !! To insult you !!! And kick you out of the bus or the train and you can\\'t do nothing at all against them !!! In other words ?? They are A Big Cruel Filthy Mafia !!! Gangsters !!! Nothing else !!! That\\'s what I feel on this MTA Company !!! I wish I had another choice to use other companies cervices in a reasonable prices to go to work and back !! But there is non of them !!!', 'The Q65 bus tonight has no screens on to show the routes, too dark to see any signs, missed a stop, had to shout twice for back door to get off a stop. Why  are we paying for services at night anyway?']\n\n\n\n\nCode\nvectorizer=CountVectorizer(min_df=0.001)   \n\n# RUN COUNT VECTORIZER ON OUR COURPUS \ntext_Xs  =  vectorizer.fit_transform(corpus)   \ntext_X=np.array(text_Xs.todense())\n\n#CONVERT TO ONE-HOT VECTORS\nmaxs=np.max(text_X,axis=0)\ntext_X=np.ceil(text_X/maxs)\n\n# DOUBLE CHECK \nprint(text_X.shape,y2.shape)\nprint(\"DATA POINT-0:\",text_X[0,0:10],\"  y2 =\",y2[0])\n\n\n(2435, 5804) (2435,)\nDATA POINT-0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]   y2 = 4\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\ntest_ratio=0.2\n\n# SPLIT ARRAYS OR MATRICES INTO RANDOM TRAIN AND TEST SUBSETS.\ntext_x_train, text_x_test, text_y_train, text_y_test = train_test_split(text_X, y2, test_size=test_ratio, random_state=0)\ntext_y_train=text_y_train.flatten()\ntext_y_test=text_y_test.flatten()\n\nprint(\"x_train.shape        :\",text_x_train.shape)\nprint(\"y_train.shape        :\",text_y_train.shape)\n\nprint(\"X_test.shape     :\",text_x_test.shape)\nprint(\"y_test.shape     :\",text_y_test.shape)\nprint(text_y_train[0:100])\n\n\nx_train.shape       : (1948, 5804)\ny_train.shape       : (1948,)\nX_test.shape        : (487, 5804)\ny_test.shape        : (487,)\n[1 3 4 3 2 1 2 1 4 1 1 5 1 4 2 1 1 2 3 2 4 1 1 1 2 4 1 1 5 4 2 1 1 1 5 1 2\n 2 1 4 5 1 2 1 4 1 1 1 1 2 1 4 5 1 3 1 1 1 4 3 1 4 3 3 3 1 3 4 1 1 3 1 3 3\n 1 5 1 4 4 1 5 3 1 3 3 1 5 3 1 2 1 5 2 4 1 1 4 2 5 3]\n\n\n\n\nCode\nfrom sklearn import model_selection\n\ndef report(y,ypred):\n      #ACCURACY COMPUTE \n      print(\"Accuracy:\",accuracy_score(y, ypred)*100)\n      print(\"Number of mislabeled points out of a total %d points = %d\"\n            % (y.shape[0], (y != ypred).sum()))\n\ndef print_model_summary(model):\n      # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n      text_yp_train = model.predict(text_x_train)\n      text_yp_test = model.predict(text_x_test)\n\n      print(\"ACCURACY CALCULATION\\n\")\n\n      print(\"TRAINING SET:\")\n      report(text_y_train,text_yp_train)\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      report(text_y_test,text_yp_test)\n\n      print(\"\\nCHECK FIRST 20 PREDICTIONS\")\n      print(\"TRAINING SET:\")\n      print(text_y_train[0:20])\n      print(text_yp_train[0:20])\n      print(\"ERRORS:\",text_yp_train[0:20]-text_y_train[0:20])\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      print(text_y_test[0:20])\n      print(text_yp_test[0:20])\n      print(\"ERRORS:\",text_yp_test[0:20]-text_y_test[0:20])\n\n\n\n\nCode\nmodel = MultinomialNB()\nmodel.fit(text_x_train,text_y_train)\nprint_model_summary(model)\n\n\nACCURACY CALCULATION\n\nTRAINING SET:\nAccuracy: 89.11704312114989\nNumber of mislabeled points out of a total 1948 points = 212\n\nTEST SET (UNTRAINED DATA):\nAccuracy: 52.361396303901444\nNumber of mislabeled points out of a total 487 points = 232\n\nCHECK FIRST 20 PREDICTIONS\nTRAINING SET:\n[1 3 4 3 2 1 2 1 4 1 1 5 1 4 2 1 1 2 3 2]\n[1 3 4 4 2 1 4 1 4 1 1 5 1 4 1 1 1 4 3 1]\nERRORS: [ 0  0  0  1  0  0  2  0  0  0  0  0  0  0 -1  0  0  2  0 -1]\n\nTEST SET (UNTRAINED DATA):\n[5 1 1 2 1 2 3 2 4 1 5 4 1 5 5 1 1 2 3 4]\n[4 1 1 3 1 4 4 1 4 1 4 4 1 4 4 1 1 1 3 1]\nERRORS: [-1  0  0  1  0  2  1 -1  0  0 -1  0  0 -1 -1  0  0 -1  0 -3]\n\n\n\n\nCode\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ncm = confusion_matrix(text_y_train, model.predict(text_x_train), labels=model.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\ndisp.plot()\nplt.show()\n\n\n\n\n\n\n\nCode\ncm = confusion_matrix(text_y_test, model.predict(text_x_test), labels=model.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\ndisp.plot()\nplt.show()\n\n\n\n\n\nThese confusion matrices and accuracy scores show that the Naive Bayes method was very effective on the training dataset, and significantly less effective for the test set. An accuracy score of almost 90 is worth paying attention to, and the confusion matrix backs that up with large values on the diagonal. However, for the test data, this accuracy score dropped to just over 50. It is reasonable to conclude that this algorithm has its shortcomings when attempting to predict new data. Much of that is likely due to a lack of an established pattern within the data. In other words, the relationship between ratings and their respective text review is not consistent from one sample to another. Nonetheless, we can see from the final confusion matrix that the largest prediction value for each true value is on the diagonal for all labels, which does indicate that this is better than random chance."
  },
  {
    "objectID": "5000-website/naive_bayes/naive_bayes.html#conclusions",
    "href": "5000-website/naive_bayes/naive_bayes.html#conclusions",
    "title": "Naive Bayes",
    "section": "Conclusions",
    "text": "Conclusions\nWhile there are clear shortcomings with the Naive Bayes methodology for both our record and text data, the ability to make predictions based on similarities within the feature variables is clearly there. Our predictions were more likely than not to match up with actual values.\nThis result tells us that there is value in looking at demographic data for predicting method of transportation for commuting to work. Thus, there are certain groups of people who are more affected by public transit efficiency and ubiquity than others. In terms of social value, that is an important distinction to make."
  },
  {
    "objectID": "5000-website/naive_bayes/naive_bayes.html#footnotes",
    "href": "5000-website/naive_bayes/naive_bayes.html#footnotes",
    "title": "Naive Bayes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“1.9. naive Bayes,” scikit, https://scikit-learn.org/stable/modules/naive_bayes.html (accessed Nov. 2, 2023).↩︎"
  },
  {
    "objectID": "5000-website/arm/arm.html",
    "href": "5000-website/arm/arm.html",
    "title": "ARM",
    "section": "",
    "text": "Build out your website tab for “ARM”"
  },
  {
    "objectID": "5000-website/cleaning/cleaning.html",
    "href": "5000-website/cleaning/cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "To see all cleaned data click here"
  },
  {
    "objectID": "5000-website/cleaning/cleaning.html#footnotes",
    "href": "5000-website/cleaning/cleaning.html#footnotes",
    "title": "Data Cleaning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Ridership Report.” American Public Transportation Association, 21 Sept. 2023, www.apta.com/research-technical-resources/transit-statistics/ridership-report/.↩︎\nSteven Ruggles, Sarah Flood, Matthew Sobek, Danika Brockman, Grace Cooper, Stephanie Richards, and Megan Schouweiler. IPUMS USA: Version 13.0 [dataset]. Minneapolis, MN: IPUMS, 2023. https://doi.org/10.18128/D010.V13.0↩︎\n“Raw monthly ridership (no adjustments or estimates),” Raw Monthly Ridership (No Adjustments or Estimates) | FTA, https://www.transit.dot.gov/ntd/data-product/monthly-module-raw-data-release (accessed Nov. 14, 2023).↩︎\n“Metropolitan Transportation Authority - New York, NY,” Yelp, https://www.yelp.com/biz/metropolitan-transportation-authority-new-york-6 (accessed Nov. 14, 2023).↩︎\n“Metro Los Angeles - Los Angeles, CA,” Yelp, https://www.yelp.com/biz/wmata-washington (accessed Nov. 14, 2023).↩︎\n“Chicago Transit Authority - Chicago, IL,” Yelp, https://www.yelp.com/biz/metro-los-angeles-los-angeles (accessed Nov. 14, 2023).↩︎\n“Septa - Philadelphia, PA,” Yelp, https://www.yelp.com/biz/septa-philadelphia-7 (accessed Nov. 14, 2023).↩︎\n“Massachusetts Bay Transportation Authority - Boston, MA,” Yelp, https://www.yelp.com/biz/massachusetts-bay-transportation-authority-boston (accessed Nov. 14, 2023).↩︎\n“WMATA - Washington, DC, DC,” Yelp, https://www.yelp.com/biz/wmata-washington (accessed Nov. 2, 2023).↩︎\n“Bart - Bay Area Rapid Transit - Oakland, CA,” Yelp, https://www.yelp.com/biz/bart-bay-area-rapid-transit-oakland-2 (accessed Nov. 2, 2023).↩︎\nBarrero, Jose Maria, et al. Why Working from Home Will Stick, 2021, https://doi.org/10.3386/w28731.↩︎"
  },
  {
    "objectID": "5000-website/data/data.html",
    "href": "5000-website/data/data.html",
    "title": "Data",
    "section": "",
    "text": "Click here to access the Data folder of Github repository\n\nBelow are the tabs for this portion of the report:\n\nData Gathering\nData Cleaning\nExploratory Data Analysis"
  },
  {
    "objectID": "5000-website/gathering/gathering.html",
    "href": "5000-website/gathering/gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "To see all raw data gathered click here"
  },
  {
    "objectID": "5000-website/gathering/gathering.html#quarterly-and-annual-ridership-totals-by-mode-of-transportation-1",
    "href": "5000-website/gathering/gathering.html#quarterly-and-annual-ridership-totals-by-mode-of-transportation-1",
    "title": "Data Gathering",
    "section": "Quarterly and Annual Ridership Totals by Mode​ of Transportation 1",
    "text": "Quarterly and Annual Ridership Totals by Mode​ of Transportation 1\nThe initial piece of data that was gathered comes from the American Public Transportation Association, and can serve as an introductory synopsis of the state of public transit ridership over time. This gives a broad view of quarterly ridership across the entire country from 1990 onward. Thus, this data has been chosen for the potential of setting the stage for the problem which we intend to explore.\nThe raw data and methodology for how it was obtained can be found using this link: https://www.apta.com/research-technical-resources/transit-statistics/ridership-report/\nThe data itself can be downloaded using this link: https://www.apta.com/wp-content/uploads/APTA-Ridership-by-Mode-and-Quarter-1990-Present.xlsx\nTo download this data, I used an R API tool, which saves the data in Excel format. Below is the code for this action and a screenshot of the raw data to illustrate its form upon download:\n\n\nCode\nlibrary(readxl)\nlibrary(httr)\nurl1&lt;-'https://www.apta.com/wp-content/uploads/APTA-Ridership-by-Mode-and-Quarter-1990-Present.xlsx'\nGET(url1, write_disk(tf &lt;- tempfile(pattern = \"APTA-Ridership-by-Mode-and-Quarter-1990-Present\", fileext = \".xlsx\", tmpdir = \"../data\")))\ndf &lt;- read_excel(tf, 2L)\nstr(df)\n\n\n\n\n\nQuarterly and Annual Ridership Totals by Mode​ of Transportation"
  },
  {
    "objectID": "5000-website/gathering/gathering.html#census-data-for-commute-to-work-2",
    "href": "5000-website/gathering/gathering.html#census-data-for-commute-to-work-2",
    "title": "Data Gathering",
    "section": "Census Data for Commute to Work 2",
    "text": "Census Data for Commute to Work 2\nNote: Due to the size of this data, it will not be hosted on Github. The raw data can be accessed using this link.\nIn answering the question of whether or not public transit’s public service should be the paramount consideration for its efficacy, it is important to understand that it often provides service disproportionally to underprivileged groups. Therefore, a source of data that will be useful is a survey dataset from IPUMS, which has millions of survey responses from the U.S. Census Bureau. We will be using the survey data gathered from 2021, the most recent sample available, and one with a significantly large volume of responses. The main reason for obtaining this data is the presence of a “Means of transportation to work” field, which will serve as our labels for supervised learning. This can tell us the commute methods for a large sample of respondents, whom we can analyze by looking at demographic and lifestyle data to gather insights on what factors into one’s means of transportation to work.\nThis data was gathered via the IPUMS website by selecting a sample (2021) and specifying fields that will be required for further analysis. IPUMS provides numeric codes for categorical variables and to represent NA values, the meanings of which are specified on their website. This will be useful in cleaning the data. The fields for this data extract are below:\n\n\n\nSurvey Extract Fields\n\n\nAfter extracting, the data appears as the following:\n\n\n\nSurvey Raw Dataset"
  },
  {
    "objectID": "5000-website/gathering/gathering.html#public-transit-data-by-city3",
    "href": "5000-website/gathering/gathering.html#public-transit-data-by-city3",
    "title": "Data Gathering",
    "section": "Public Transit Data by City3",
    "text": "Public Transit Data by City3\nJust as not every city was equally impacted by the COVID-19 pandemic, the performance of public transit differs drastically depending on where one goes in the US. Our goal here is to assess what factors impact public transit ridership and cost-effectiveness for a city to determine any action items that can be taken to improve such metrics. Every quarter, the American Public Transit Association (APTA) conducts a Ridership Report which includes key performance indicators of public transit systems across the US. Our analysis will use data from their Third Quarter 2023 Ridership Report, which is the most recent one available at the time of writing this.\nThe observational unit for this data is not city, but rather type of transportation. Therefore, any given city may have several rows, possibly split into Agency, as multiple organizations may serve a single city to provide service to its residents. These agencies are also split by row, corresponding to the mode of transportation. Another notable aspect of this data is that it includes general information about each city, such as population and area. In this case, records have the same value in these columns for each city, so when cleaning, we can consolidate records by city and not worry about conflating differing values. This will allow us to not only focus on features of public transit systems in a vacuum, but also observe characteristics of cities that may lead to various public transit phenomena. A screenshot of the raw data gathered from APTA is below:\n\n\n\nPublic Transit Data by City"
  },
  {
    "objectID": "5000-website/gathering/gathering.html#yelp-reviews",
    "href": "5000-website/gathering/gathering.html#yelp-reviews",
    "title": "Data Gathering",
    "section": "Yelp Reviews",
    "text": "Yelp Reviews\nGauging public sentiment regarding public transit systems can be a great way to analyze the relationship between said system and the residents of its respective city. Regardless of external factors, consumer dissatisfaction of a mode of transportation could greatly influence its usage when other methods are readily available for many. Thus, these datasets will feature Yelp reviews of the top seven most used public transit systems in the US, as measured by the data gathered from the American Public Transit Association. These public transit systems are:\n\nMetropolitan Transit Authority (New York City)4\nLos Angeles County Metropolitan Transportation Authority5\nChicago Transit Authority6\nSoutheastern Pennsylvania Transportation Authority (Philadelphia)7\nMassachusetts Bay Transportation Authority (Boston)8\nBay Area Rapid Transit (San Francisco Bay Area)9\nWashington Metropolitan Area Transit Authority (Washington, D.C.)10\n\nThis data will include the date of review, the exact text, and the associated numerical rating (1-5 stars). Gathering labeled text data will be invaluable for Naive Bayes classification in the future. To accomplish this, I will use the BeautifulSoup package in Python, which facilitates web scraping via HTML codes. Since the reviews span several pages, it is necessary to iterate over each page to obtain every review available to us. Therefore, I adapted a generalized function for both reading in the data and storing it as a pandas dataframe.11 The code for that function is below:\n\n\nCode\nimport pandas as pd\nimport csv\nimport requests\nfrom bs4 import BeautifulSoup\ndef get_yelp(url, pages):   \n    r = requests.get(url)\n    soup = BeautifulSoup(r.text, 'html.parser')\n    hold1 = soup.find(string='Recommended Reviews')\n    if hold1 is not None:\n        reviews = hold1.find_parent('section')\n    num_rating1 = []\n    review_date1 = []\n    review_text1 = []\n    for review in reviews.select('div[aria-label$=\"star rating\"]'):\n        num_rating1.append(review['aria-label'])\n        review_date1.append(review.find_next('span').text)\n        hold = review.find_next('span', lang=True)\n        if hold is None:\n            review_text1.append(\"NA\")\n        else:\n            review_text1.append(hold.text)\n\n    for i in range(1,pages):\n        r = requests.get(url + '?start=' + str(i) + '0')\n        soup = BeautifulSoup(r.text, 'html.parser')\n        hold1 = soup.find(string='Recommended Reviews')\n        if hold1 is not None:\n            reviews = hold1.find_parent('section')\n        for review in reviews.select('div[aria-label$=\"star rating\"]'):\n            num_rating1.append(review['aria-label'])\n            review_date1.append(review.find_next('span').text)\n            hold = review.find_next('span', lang=True)\n            if hold is None:\n                review_text1.append(\"NA\")\n            else:\n                review_text1.append(hold.text)\n    reviews = pd.DataFrame(list(zip(num_rating1,review_date1,review_text1)))\n    return(reviews)\n\n\nUpon creating and running the function above, we can now call it using our seven most used public transit systems in the US. The inputs for this function are the following:\n\nThe URL for the first page on Yelp\nThe number of pages of reviews\n\nThe code for calling this function is below, with an example of the output attached:\n\n\nCode\nget_yelp('https://www.yelp.com/biz/metropolitan-transportation-authority-new-york-6',14).to_csv('../data/yelp_reviews/mta_reviews.csv')\nget_yelp('https://www.yelp.com/biz/metro-los-angeles-los-angeles',18).to_csv('../data/yelp_reviews/la_reviews.csv')\nget_yelp('https://www.yelp.com/biz/chicago-transit-authority-chicago-6',38).to_csv('../data/yelp_reviews/cta_reviews.csv')\nget_yelp('https://www.yelp.com/biz/septa-philadelphia-7',10).to_csv('../data/yelp_reviews/septa_reviews.csv')\nget_yelp('https://www.yelp.com/biz/massachusetts-bay-transportation-authority-boston',34).to_csv('../data/yelp_reviews/mbta_reviews.csv')\nget_yelp('https://www.yelp.com/biz/bart-bay-area-rapid-transit-oakland-2',101).to_csv('../data/yelp_reviews/bart_reviews.csv')\nget_yelp('https://www.yelp.com/biz/wmata-washington',10).to_csv('../data/yelp_reviews/wmata_reviews.csv')\n\n\n\n\n\nWMATA Yelp Reviews"
  },
  {
    "objectID": "5000-website/gathering/gathering.html#remote-work-trends-12",
    "href": "5000-website/gathering/gathering.html#remote-work-trends-12",
    "title": "Data Gathering",
    "section": "Remote Work Trends 12",
    "text": "Remote Work Trends 12\nIt is reasonable to hypothesize that one of the main factors in public transit usage is people commuting to and from work. The term “rush hour” is a seemingly daily phrase, meaning the times in the morning and evening at which most people go to or return from their occupation. Thus, when COVID-19 struck and many workers were no longer expected to go to work in-person, the need for public transportation decreased drastically.\nIn the years since, remote work has been a topic of controversy. Many workers enjoy the benefits of privacy and the added time of not having to commute, while employers often cite advantages of being on-site even in office jobs. While in-person work has rebounded recently, much like public transit usage, it has not nearly returned to the prevalence of prior to the pandemic. Therefore, understanding trends surrounding remote work can provide insights on how to analyze public transportation trends.\nWFH Research has exhaustive data sets regarding remote work information. For the purposes of this project, we will take into account three data sets. To better understand the controversial aspects of remote work, the first two data sets contain survey information from (a) employers and (b) workers on what they desire in terms of average remote work days per week. The third data set provides time series information on the amount of working from home (percent of full paid days) for large cities. Screenshots of the raw data are shown below:\n\n\n\nRemote Work Desires of Employers\n\n\n\n\n\nRemote Work Desires of Workers\n\n\n\n\n\nRemote Work Percentages by City"
  },
  {
    "objectID": "5000-website/gathering/gathering.html#footnotes",
    "href": "5000-website/gathering/gathering.html#footnotes",
    "title": "Data Gathering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Ridership Report.” American Public Transportation Association, 21 Sept. 2023, www.apta.com/research-technical-resources/transit-statistics/ridership-report/.↩︎\nSteven Ruggles, Sarah Flood, Matthew Sobek, Danika Brockman, Grace Cooper, Stephanie Richards, and Megan Schouweiler. IPUMS USA: Version 13.0 [dataset]. Minneapolis, MN: IPUMS, 2023. https://doi.org/10.18128/D010.V13.0↩︎\n“Raw monthly ridership (no adjustments or estimates),” Raw Monthly Ridership (No Adjustments or Estimates) | FTA, https://www.transit.dot.gov/ntd/data-product/monthly-module-raw-data-release (accessed Nov. 14, 2023).↩︎\n“Metropolitan Transportation Authority - New York, NY,” Yelp, https://www.yelp.com/biz/metropolitan-transportation-authority-new-york-6 (accessed Nov. 14, 2023).↩︎\n“Metro Los Angeles - Los Angeles, CA,” Yelp, https://www.yelp.com/biz/wmata-washington (accessed Nov. 14, 2023).↩︎\n“Chicago Transit Authority - Chicago, IL,” Yelp, https://www.yelp.com/biz/metro-los-angeles-los-angeles (accessed Nov. 14, 2023).↩︎\n“Septa - Philadelphia, PA,” Yelp, https://www.yelp.com/biz/septa-philadelphia-7 (accessed Nov. 14, 2023).↩︎\n“Massachusetts Bay Transportation Authority - Boston, MA,” Yelp, https://www.yelp.com/biz/massachusetts-bay-transportation-authority-boston (accessed Nov. 14, 2023).↩︎\n“WMATA - Washington, DC, DC,” Yelp, https://www.yelp.com/biz/wmata-washington (accessed Nov. 2, 2023).↩︎\n“Bart - Bay Area Rapid Transit - Oakland, CA,” Yelp, https://www.yelp.com/biz/bart-bay-area-rapid-transit-oakland-2 (accessed Nov. 2, 2023).↩︎\n“Is it actually possible to scrape reviews from yelp with Beautifulsoup?,” Reddit, https://www.reddit.com/r/learnpython/comments/d5a71p/comment/f0mi983/?utm_source=share&utm_medium=web2x&context=3 (accessed Dec. 5, 2023).↩︎\nBarrero, Jose Maria, et al. Why Working from Home Will Stick, 2021, https://doi.org/10.3386/w28731.↩︎"
  },
  {
    "objectID": "5000-website/index.html",
    "href": "5000-website/index.html",
    "title": "About the Author",
    "section": "",
    "text": "Lab 1.1.3\n\n\n\n\n\n\nJosh Sweren is a graduate student in the Data Science and Analytics program at Georgetown University. He received his B.S. in Statistics with minors in Computer Science and Mathematics from the George Washington University in 2020. Since graduating, he has remained in the Washington, D.C. area, and has worked in government consulting for over three years. He currently works as a Business Analyst for an IT contractor, serving under a contract with the United States Department of Agriculture. Originally from New York, Mr. Sweren is eager to learn about Machine Learning and Deep Learning, as well as the broader topics of data visualization and communication. Outside of work, he enjoys playing and watching sports, as well as doing puzzles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Current Coursework\n\n\n\nDSAN 5500: Data Structures, Objects, and Algorithms\n\n\n\nDSAN 5600: Applied Time Series for Data Science\n\n\n\n\n  Relevant Prior Coursework\n\n\n\nDSAN 5000: Data Science and Analytics\n\n\n\nDSAN 5100: Probabilistic Modeling and Statistical Computing\n\n\n\nDSAN 5200: Advanced Data Visualization\n\n\n\nDSAN 5300: Statistical Learning\n\n\n\nDSAN 6000: Big Data and Cloud Computing\n\n\n\nDSAN 6600: Neural Nets and Deep Learning"
  },
  {
    "objectID": "5100-website/eda/eda.html",
    "href": "5100-website/eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Build out your website tab for exploratory data analysis"
  },
  {
    "objectID": "5100-website/eda/eda.html#quick-look-at-the-data",
    "href": "5100-website/eda/eda.html#quick-look-at-the-data",
    "title": "Data Exploration",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\nprint(tips)\n\n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]"
  },
  {
    "objectID": "5100-website/eda/eda.html#basic-visualization",
    "href": "5100-website/eda/eda.html#basic-visualization",
    "title": "Data Exploration",
    "section": "Basic visualization",
    "text": "Basic visualization\n\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n\nplt.show()"
  },
  {
    "objectID": "5100-website/clustering/clustering.html",
    "href": "5100-website/clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Build out your website tab for “clustering”"
  },
  {
    "objectID": "5100-website/index.html",
    "href": "5100-website/index.html",
    "title": "DSAN-5100: Introduction",
    "section": "",
    "text": "See the following link for more information about the author: about me\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nHIGHLY RECOMMENDED\n\nIt is highly recommended that you build your website using .ipynb files and NOT .qmdfiles\nFunctionally the two formats are basically identical, i.e. they are just Markdown + Code\nHowever there is ONE MAJOR DIFFERENCE, i.e. .ipynb stores the code outputs in the meta-data of the file\n\nThis means you ONLY HAVE TO RUN THE CODE ONCE with .ipynb\n.qmd will run the code every time you build the website, which can be very slow\n\nThere are caching options for .qmd, however, they are “messier” that just using .ipynb\n\nNote: .qmd is fine if there is no code, in which case it is basically just a Markdown file\n\nConverting between the two\n\nYou can switch between the two formats using\nquarto convert clustering.qmd this will output a .ipynb version called clustering.ipynb\nquarto convert eda.ipynb this will output a .qmd version called eda.qmd\n\nYOU CAN RUN R CODE IN VSC WITH .ipynb, see the following link\n\nhttps://saturncloud.io/blog/how-to-use-jupyter-r-kernel-with-visual-studio-code/\n\nIt is possible, but NOT RECOMMENDED, to mix Python and R code in the same file\n\nIMPORTANT ASIDE\n\nA .ipynb file is simply a JSON file with a specialized structural format\nYou can see this by running more eda/eda.ipynb from the command line\nWhich will output the following;\n\n\nTIP FOR MAC USERS\n\ncommand+control+shift+4 is very useful on a mac.\n\nIt takes a screenshot and saves it to the clip-board\n\nThe following VSC extension allows you to paste images from the clip-board with alt+command+v.\n\ntab is your best friend when using the command line, since it does auto-completion\nopen ./path_to_file will open any file or directory from the command line"
  },
  {
    "objectID": "5600-website/index.html",
    "href": "5600-website/index.html",
    "title": "5600-website",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "index.html#project-portfolio",
    "href": "index.html#project-portfolio",
    "title": "Joshua Sweren",
    "section": "Project Portfolio",
    "text": "Project Portfolio\n\n \n\n\nMonitoring the Metro\n\n\nAn ETL pipeline and Streamlit visualization using Python to analyze the punctuality and wait times of Washington, D.C.’s transit system.\n\n\n  \n\n\nBeyond the Pitch: Perception vs. Performance\n\n\nA big data project using NLP tools in AWS and PySpark to analyze Reddit activity during soccer tournaments in summer 2024.\n\n\n  \n\n\nPublic Transit Ridership in the Wake of COVID-19\n\n\nA time series analysis of recent national public transit ridership trends, incorporating deep learning and multivariate techniques.\n\n\n  \n\n\nAmerica’s Pastime to a Global Game\n\n\nA Data Visualization project evaluating the evolving demographics of Major League Baseball using advanced packages in Python and R."
  },
  {
    "objectID": "index.html#current-coursework",
    "href": "index.html#current-coursework",
    "title": "Joshua Sweren",
    "section": "Current Coursework",
    "text": "Current Coursework\n\nDSAN-6300: SQL and Database Systems\nDSAN-6750: Geographic Information Systems (GIS) for Spatial Data Science"
  },
  {
    "objectID": "index.html#prior-coursework",
    "href": "index.html#prior-coursework",
    "title": "Joshua Sweren",
    "section": "Prior Coursework",
    "text": "Prior Coursework\n\nDSAN-5500: Data Structures, Objects, and Algorithms\nDSAN-5600: Applied Time Series for Data Science\nDSAN-6000: Big Data and Cloud Computing\nDSAN-6600: Neural Nets and Deep Learning\nDSAN-5200: Advanced Data Visualization\nDSAN-5300: Statistical Learning\nDSAN-5000: Data Science and Analytics\nDSAN-5100: Probabilistic Modeling and Statistical Computing"
  }
]